{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, TypeAlias\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample: TypeAlias = list[int | float]\n",
    "Data: TypeAlias = list[Sample]\n",
    "\n",
    "Target: TypeAlias = int | float\n",
    "Targets: TypeAlias = list[Target]\n",
    "\n",
    "Weights: TypeAlias = list[list[float]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class ActivationBase(ABC):\n",
    "    @abstractmethod\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        \"\"\"Apply the activation function to an layer output\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, x: Sample):\n",
    "        pass\n",
    "\n",
    "#######################################\n",
    "\n",
    "\n",
    "class ReLU(ActivationBase):\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def derivative(self, x: Sample):\n",
    "        return self.calc(x=x)\n",
    "\n",
    "\n",
    "class Sigmoid(ActivationBase):\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x: Sample):\n",
    "        return x * (1-x)\n",
    "\n",
    "\n",
    "class Softmax(ActivationBase):\n",
    "    \"\"\"returns model 'probabilities' for each class\"\"\"\n",
    "\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        \n",
    "        # optimization: make numbers in an array from -inf to 0 because of a np.exp growing\n",
    "        # and returns an array of floats from 0.0 to 1.0\n",
    "        max_value = np.max(x)\n",
    "        x -= max_value\n",
    "\n",
    "        exp_values = np.exp(x)\n",
    "        return exp_values / np.sum(exp_values)\n",
    "    \n",
    "    def derivative(self, x: Sample):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "\n",
    "class LossBase(ABC):\n",
    "    @abstractmethod\n",
    "    def calc(self, x: Sample, y: Target) -> float:\n",
    "        \"\"\"Apply the loss function to an output layer\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class MSELoss(LossBase):\n",
    "    \"\"\"For regression\"\"\" \n",
    "    def calc(self, x: Sample, y: Target) -> float:\n",
    "\n",
    "        loss = (y - x) ** 2\n",
    "\n",
    "        return np.mean(loss)\n",
    "\n",
    "\n",
    "class CrossEntropy(LossBase):\n",
    "    \"\"\"For classification\"\"\"\n",
    "    def calc(self, x: Sample, y: Target) -> float:\n",
    "        return -np.sum(y * np.log(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.39262147e-44 7.31058579e-01 9.74950551e-35 2.68941421e-01]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = [1, 100, 22, 99]\n",
    "\n",
    "f = Softmax()\n",
    "\n",
    "b = f.calc(a)\n",
    "print(b)\n",
    "print(sum(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data: Data, targets: Targets) -> None:\n",
    "        self.data: Data = data\n",
    "        self._len = len(data)\n",
    "        self.targets: Targets = targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._len\n",
    "    \n",
    "    def __getitem__(self, index) -> Sample:\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_inputs: int, n_neurons: int, activation: ActivationBase) -> None:\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        \n",
    "        self.weights = self._init_weights()\n",
    "        self.bias = self._init_bias()\n",
    "        self.output = []\n",
    "\n",
    "        self.activation = activation\n",
    "    \n",
    "    def _init_weights(self) -> list[float]:\n",
    "        scale = 1/max(1., (2+2)/2.)\n",
    "        limit = np.sqrt(3.0 * scale)\n",
    "\n",
    "        # return np.random.randn(self.n_neurons, self.n_inputs) #* 0.1\n",
    "    \n",
    "        weights = np.random.uniform(-limit, limit, size=(self.n_neurons, self.n_inputs))\n",
    "        return weights\n",
    "    \n",
    "    def _init_bias(self) -> list[float]:\n",
    "        return np.random.randn(1)\n",
    "    \n",
    "    def forward(self, inputs) -> None:\n",
    "        # print(f\"{inputs = }\")\n",
    "        # print(f\"{self.weights = }\")\n",
    "\n",
    "        output = np.matmul( self.weights, inputs.T)\n",
    "\n",
    "        output += self.bias\n",
    "\n",
    "        self.output = self.activation.calc(output)\n",
    "        # print(f\"{self.output = }\")\n",
    "    \n",
    "\n",
    "    # def backward(self, grad, learning_rate: float, output):\n",
    "        \n",
    "    #     # print(f\"{self.weights = }\")\n",
    "    #     print(f\"{output = }\")\n",
    "    #     print(f\"{grad = }\")\n",
    "\n",
    "    #     print()\n",
    "\n",
    "    #     weights_new = self.weights - grad * output * learning_rate\n",
    "    #     # print(f\"{weights_new = }\")\n",
    "    #     # print('\\n\\n')\n",
    "\n",
    "    #     self.weights = weights_new\n",
    "\n",
    "\n",
    "Layers: TypeAlias = list[Linear]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers: Layers, loss: LossBase):\n",
    "        self.layers = layers\n",
    "        self._layers_len = len(layers)\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, dataset: Dataset, n_epoch: int = 1, learning_rate: float = 0.01, verbose: bool = True) -> list[float]:\n",
    "        losses_by_epoch = []\n",
    "\n",
    "        range_epoch = range(n_epoch)\n",
    "        if verbose:\n",
    "            range_epoch = tqdm(range_epoch, desc=\"epochs\", position=0)\n",
    "\n",
    "        for epoch in range_epoch:\n",
    "            epoch_losses = []\n",
    "\n",
    "            for i,sample in enumerate(dataset):\n",
    "                sample = np.array(sample)\n",
    "                # sample = sample.reshape(1,len(sample))\n",
    "\n",
    "\n",
    "                # Forward pass\n",
    "                self.layers[0].forward(inputs=sample) # input layer\n",
    "                for j in range(1, self._layers_len):\n",
    "                    self.layers[j].forward(inputs=self.layers[j-1].output)\n",
    "\n",
    "                target = dataset.targets[i]\n",
    "                \n",
    "\n",
    "                # Calc loss\n",
    "                output_error = self.calc_loss(target=target)\n",
    "                epoch_losses.append(output_error)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "                # Backward pass\n",
    "                # backward from output to input layer\n",
    "                # propagate gradients using chain rule\n",
    "\n",
    "                delta =  self.layers[-2].activation.derivative(x=self.layers[-1].output) * output_error\n",
    "\n",
    "                for i in range(self._layers_len - 2, -1, -1):\n",
    "                    # print(\"weights\", self.layers[i+1].weights)\n",
    "                    # print(\"delta\", delta)\n",
    "\n",
    "                    error = np.dot(self.layers[i+1].weights.T, delta)\n",
    "                    delta =  self.layers[i].activation.derivative(x=self.layers[i].output) * error\n",
    "\n",
    "                    \n",
    "                    # print(f\"weights {i+1}\", self.layers[i+1].weights)\n",
    "                    self.layers[i+1].weights += np.dot(delta, self.layers[i].output.T) * learning_rate\n",
    "                    # print(f\"weights {i+1}\", self.layers[i+1].weights)\n",
    "\n",
    "                    \n",
    "            mean_loss = np.mean(epoch_losses)\n",
    "            losses_by_epoch.append(mean_loss)\n",
    "        \n",
    "        return losses_by_epoch\n",
    "    \n",
    "    def predict(self, sample: Sample) -> list[float]:\n",
    "        sample = np.array(sample)\n",
    "        sample = sample.reshape(1,len(sample))\n",
    "\n",
    "        self.layers[0].forward(inputs=sample)\n",
    "                \n",
    "        for j in range(1, self._layers_len):\n",
    "            self.layers[j].forward(inputs=self.layers[j-1].output)\n",
    "        \n",
    "        predict = self.layers[-1].output\n",
    "        return predict\n",
    "    \n",
    "    def calc_loss(self, target: Target) -> float:\n",
    "        output_layer = self.layers[-1]\n",
    "        output = output_layer.output\n",
    "\n",
    "        loss = self.loss.calc(x=output, y=target)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def set_weights(self, weights: Weights) -> None:\n",
    "        for w,layer in zip(weights, self.layers):\n",
    "            layer.weights = w\n",
    "\n",
    "    @property\n",
    "    def weights(self) -> Weights:\n",
    "        weights = [layer.weights for layer in self.layers]\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\n",
    "    # [1,2,3,4],\n",
    "    [4,3,2,1]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# X_train = [\n",
    "#     [[1,2,3], [1,2,3], [1,2,3]], # photo\n",
    "# ]\n",
    "\n",
    "y_train = [1,]\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = Dataset(data=X_train, targets=y_train)\n",
    "# val_dataset = Dataset(data=X_val, targets=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6666666666666665]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input and output layers must be with equal numbers\n",
    "\n",
    "layers = [\n",
    "    Linear(4,13, activation=Sigmoid()),\n",
    "    Linear(13,4, activation=Sigmoid()),\n",
    "    Linear(4,2, activation=Sigmoid()),\n",
    "    Linear(2,4, activation=Sigmoid()),\n",
    "    Linear(4,2, activation=Sigmoid()),\n",
    "    Linear(2,100, activation=Sigmoid()),\n",
    "    Linear(100,1, activation=Sigmoid()),\n",
    "    Linear(1,1, activation=Sigmoid()),\n",
    "    Linear(1,1, activation=Softmax()),\n",
    "]\n",
    "\n",
    "model = Model(layers=layers, loss=MSELoss())\n",
    "\n",
    "\n",
    "model.fit(dataset=train_dataset, n_epoch=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.87616528,  0.8770434 , -1.07185091, -0.3473528 ,  0.07240868,\n",
       "        -0.0397271 ,  0.04991074, -1.00090593,  0.99031344,  1.10922691,\n",
       "         0.56103235, -0.71546477,  1.19246648],\n",
       "       [ 0.29093634,  0.58343993,  0.06555826,  0.26530925, -0.87390265,\n",
       "        -0.76867888, -0.39083652, -0.94488054,  0.90105738,  0.65674802,\n",
       "         0.83505957, -0.72995415, -1.15744424],\n",
       "       [ 0.59840803,  0.97942934, -0.92458376, -1.03876575, -0.6212136 ,\n",
       "        -0.08884953,  0.68199734,  0.25303518, -0.47230019,  1.11874919,\n",
       "         0.13748123,  0.1428907 ,  0.72724899],\n",
       "       [-0.09438416, -0.39631527,  0.96266206,  0.61768845, -0.42897597,\n",
       "         1.05387946,  0.50110398,  1.00928054, -0.98790323,  0.02746748,\n",
       "         0.95923375, -0.06502083, -0.64327335]])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9391043  0.00322892 0.00930119 0.94782751 0.91798714 0.01339995\n",
      " 0.00372608 0.00276383 0.03399139 0.96232475 0.86889642 0.67765143\n",
      " 0.00543836]\n",
      "\n",
      "[0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (13,) and (4,) not aligned: 13 (dim 0) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[369], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[366], line 58\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28mprint\u001b[39m(D[i])\n\u001b[0;32m---> 58\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\n\u001b[1;32m     61\u001b[0m mean_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(epoch_losses)\n\u001b[1;32m     62\u001b[0m losses_by_epoch\u001b[38;5;241m.\u001b[39mappend(mean_loss)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (13,) and (4,) not aligned: 13 (dim 0) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "model.fit(dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset['data'], dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = set(y)\n",
    "labels_len = len(labels)\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1]]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1 = []\n",
    "\n",
    "for i in y:\n",
    "    l = [0] * labels_len\n",
    "    l[i] = 1\n",
    "    y_1.append(l)\n",
    "y_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y_1, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(data=X_train, targets=y_train)\n",
    "val_dataset = Dataset(data=X_val, targets=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Linear(4,10, activation=ReLU()),\n",
    "    Linear(10,20, activation=ReLU()),\n",
    "    Linear(20,100, activation=ReLU()),\n",
    "    Linear(100,10, activation=ReLU()),\n",
    "    Linear(10,3, activation=Softmax()),\n",
    "]\n",
    "\n",
    "model = Model(layers=layers, loss=MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:   7%|▋         | 73/1000 [00:00<00:10, 91.19it/s]/var/folders/07/5w1xtr897qz76752j_ppn7nw0000gn/T/ipykernel_18673/1056889922.py:48: RuntimeWarning: invalid value encountered in multiply\n",
      "  delta =  self.layers[i].activation.derivative(x=self.layers[i].output) * error\n",
      "epochs: 100%|██████████| 1000/1000 [00:11<00:00, 84.06it/s]\n"
     ]
    }
   ],
   "source": [
    "losses = model.fit(dataset=train_dataset, n_epoch=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phil/micromamba/envs/bio/lib/python3.12/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/Users/phil/micromamba/envs/bio/lib/python3.12/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1, 3.5, 1.4, 0.3])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[220], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, y_train[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[208], line 72\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mforward(inputs\u001b[38;5;241m=\u001b[39msample)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers_len):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39moutput\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predict\n",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# print(f\"{inputs = }\")\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# print(f\"{self.weights = }\")\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation\u001b[38;5;241m.\u001b[39mcalc(output)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 10)"
     ]
    }
   ],
   "source": [
    "model.predict(X_train[1]), y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.23116393, 0.37761343, 0.39122264]]), [0, 1, 0])"
      ]
     },
     "execution_count": 1242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_val[1]), y_val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3416666666666667"
      ]
     },
     "execution_count": 1270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_count = 0\n",
    "all_count = 0\n",
    "\n",
    "for sample,target in zip(X_train, y_train):\n",
    "    pred = model.predict(sample=sample)\n",
    "\n",
    "    pred = np.where(pred[0] >= max(pred[0]), 1, 0)\n",
    "    # print(pred)\n",
    "    # print(target)\n",
    "\n",
    "    if list(pred) == target:\n",
    "        true_count += 1\n",
    "    all_count += 1 \n",
    "\n",
    "\n",
    "acc = true_count / all_count\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 1215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([1,0,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
