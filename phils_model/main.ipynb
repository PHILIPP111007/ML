{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, TypeAlias\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample: TypeAlias = list[int | float]\n",
    "Data: TypeAlias = list[Sample]\n",
    "\n",
    "Target: TypeAlias = int | float\n",
    "Targets: TypeAlias = list[Target]\n",
    "\n",
    "Weights: TypeAlias = list[list[float]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class ActivationBase(ABC):\n",
    "    @abstractmethod\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        \"\"\"Apply the activation function to an layer output\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, x: Sample):\n",
    "        pass\n",
    "\n",
    "#######################################\n",
    "\n",
    "\n",
    "class ReLU(ActivationBase):\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def derivative(self, x: Sample):\n",
    "        return self.calc(x=x)\n",
    "\n",
    "\n",
    "class Sigmoid(ActivationBase):\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x: Sample):\n",
    "        return x * (1-x)\n",
    "\n",
    "\n",
    "class Softmax(ActivationBase):\n",
    "    \"\"\"returns model 'probabilities' for each class\"\"\"\n",
    "\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        \n",
    "        # optimization: make numbers in an array from -inf to 0 because of a np.exp growing\n",
    "        # and returns an array of floats from 0.0 to 1.0\n",
    "        max_value = np.max(x)\n",
    "        x -= max_value\n",
    "\n",
    "        exp_values = np.exp(x)\n",
    "        return exp_values / np.sum(exp_values)\n",
    "    \n",
    "    def derivative(self, x: Sample):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "\n",
    "class LossBase(ABC):\n",
    "    @abstractmethod\n",
    "    def calc(self, x: Sample, y: Target) -> float:\n",
    "        \"\"\"Apply the loss function to an output layer\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class MSELoss(LossBase):\n",
    "    \"\"\"For regression\"\"\" \n",
    "    def calc(self, x: Sample, y: Target) -> float:\n",
    "\n",
    "        loss = (y - x) ** 2\n",
    "\n",
    "        return np.mean(loss)\n",
    "\n",
    "\n",
    "class CrossEntropy(LossBase):\n",
    "    \"\"\"For classification\"\"\"\n",
    "    def calc(self, x: Sample, y: Target) -> float:\n",
    "        return -np.sum(y * np.log(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.39262147e-44 7.31058579e-01 9.74950551e-35 2.68941421e-01]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = [1, 100, 22, 99]\n",
    "\n",
    "f = Softmax()\n",
    "\n",
    "b = f.calc(a)\n",
    "print(b)\n",
    "print(sum(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data: Data, targets: Targets) -> None:\n",
    "        self.data: Data = data\n",
    "        self._len = len(data)\n",
    "        self.targets: Targets = targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._len\n",
    "    \n",
    "    def __getitem__(self, index) -> Sample:\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_inputs: int, n_neurons: int, activation: ActivationBase) -> None:\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        \n",
    "        self.weights = self._init_weights()\n",
    "        self.bias = self._init_bias()\n",
    "        self.output = []\n",
    "\n",
    "        self.activation = activation\n",
    "    \n",
    "    def _init_weights(self) -> list[float]:\n",
    "        scale = 1/max(1., (2+2)/2.)\n",
    "        limit = np.sqrt(3.0 * scale)\n",
    "\n",
    "        # return np.random.randn(self.n_neurons, self.n_inputs) #* 0.1\n",
    "    \n",
    "        weights = np.random.uniform(-limit, limit, size=(self.n_neurons, self.n_inputs))\n",
    "        return weights\n",
    "    \n",
    "    def _init_bias(self) -> list[float]:\n",
    "        return np.random.randn(1)\n",
    "    \n",
    "    def forward(self, inputs) -> None:\n",
    "        # print(f\"{inputs = }\")\n",
    "        # print(f\"{self.weights = }\")\n",
    "\n",
    "        output = np.matmul( self.weights, inputs.T)\n",
    "\n",
    "        output += self.bias\n",
    "\n",
    "        self.output = self.activation.calc(output)\n",
    "        # print(f\"{self.output = }\")\n",
    "    \n",
    "\n",
    "    # def backward(self, grad, learning_rate: float, output):\n",
    "        \n",
    "    #     # print(f\"{self.weights = }\")\n",
    "    #     print(f\"{output = }\")\n",
    "    #     print(f\"{grad = }\")\n",
    "\n",
    "    #     print()\n",
    "\n",
    "    #     weights_new = self.weights - grad * output * learning_rate\n",
    "    #     # print(f\"{weights_new = }\")\n",
    "    #     # print('\\n\\n')\n",
    "\n",
    "    #     self.weights = weights_new\n",
    "\n",
    "\n",
    "Layers: TypeAlias = list[Linear]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers: Layers, loss: LossBase, n_epoch: int = 1, learning_rate: float = 0.01, verbose: bool = True):\n",
    "        self.layers = layers\n",
    "        self._layers_len = len(layers)\n",
    "        self.loss = loss\n",
    "        \n",
    "        self.n_epoch = n_epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, dataset: Dataset) -> list[float]:\n",
    "        losses_by_epoch = []\n",
    "\n",
    "        range_epoch = range(self.n_epoch)\n",
    "        if self.verbose:\n",
    "            range_epoch = tqdm(range_epoch, desc=\"epochs\", position=0)\n",
    "\n",
    "        for epoch in range_epoch:\n",
    "            epoch_losses = []\n",
    "\n",
    "            for i,sample in enumerate(dataset):\n",
    "                sample = np.array(sample)\n",
    "                # sample = sample.reshape(1,len(sample))\n",
    "\n",
    "\n",
    "                # Forward pass\n",
    "                self.layers[0].forward(inputs=sample) # input layer\n",
    "                for j in range(1, self._layers_len):\n",
    "                    self.layers[j].forward(inputs=self.layers[j-1].output)\n",
    "\n",
    "                target = dataset.targets[i]\n",
    "                \n",
    "\n",
    "                # Calc loss\n",
    "                output_error = self.calc_loss(target=target)\n",
    "                epoch_losses.append(output_error)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "                # Backward pass\n",
    "                # backward from output to input layer\n",
    "                # propagate gradients using chain rule\n",
    "\n",
    "                delta =  self.layers[-2].activation.derivative(x=self.layers[-2].output) * output_error\n",
    "\n",
    "                print(\"output_error\", output_error)\n",
    "                print(\"delta\", delta)\n",
    "\n",
    "\n",
    "                for i in range(self._layers_len - 2, 0, -1):\n",
    "                    print(\"weights\", self.layers[i+1].weights)\n",
    "                    print(\"delta\", delta)\n",
    "\n",
    "                    error = np.dot(self.layers[i+1].weights.T, delta)\n",
    "                    delta =  self.layers[i].activation.derivative(x=self.layers[i].output) * error\n",
    "                    \n",
    "            mean_loss = np.mean(epoch_losses)\n",
    "            losses_by_epoch.append(mean_loss)\n",
    "        \n",
    "        return losses_by_epoch\n",
    "    \n",
    "    def predict(self, sample: Sample) -> list[float]:\n",
    "        sample = np.array(sample)\n",
    "        sample = sample.reshape(1,len(sample))\n",
    "\n",
    "        self.layers[0].forward(inputs=sample)\n",
    "                \n",
    "        for j in range(1, self._layers_len):\n",
    "            self.layers[j].forward(inputs=self.layers[j-1].output)\n",
    "        \n",
    "        predict = self.layers[-1].output\n",
    "        return predict\n",
    "    \n",
    "    def calc_loss(self, target: Target) -> float:\n",
    "        output_layer = self.layers[-1]\n",
    "        output = output_layer.output\n",
    "\n",
    "        loss = self.loss.calc(x=output, y=target)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def set_weights(self, weights: Weights) -> None:\n",
    "        for w,layer in zip(weights, self.layers):\n",
    "            layer.weights = w\n",
    "\n",
    "    @property\n",
    "    def weights(self) -> Weights:\n",
    "        weights = [layer.weights for layer in self.layers]\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\n",
    "    # [1,2,3,4],\n",
    "    [4,3,2,1]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# X_train = [\n",
    "#     [[1,2,3], [1,2,3], [1,2,3]], # photo\n",
    "# ]\n",
    "\n",
    "y_train = [1,]\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = Dataset(data=X_train, targets=y_train)\n",
    "# val_dataset = Dataset(data=X_val, targets=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_error 0.9826926579181234\n",
      "delta [0.24422102 0.24343578 0.24392697 0.24350237 0.24384492 0.24334213\n",
      " 0.24391293 0.24334608 0.24334231 0.24341844 0.24373281 0.24345932\n",
      " 0.24339405 0.24390948 0.24371653 0.24403863 0.24401004 0.24423316\n",
      " 0.24358916 0.24350295 0.243364   0.24420097 0.24354119 0.24422807\n",
      " 0.24339781 0.24360016 0.24414504 0.24418392 0.24406094 0.24339365\n",
      " 0.24329724 0.24336579 0.24378104 0.24374805 0.24338903 0.24402291\n",
      " 0.24328976 0.24352151 0.24349825 0.24386744 0.24420559 0.2440817\n",
      " 0.24421115 0.24401052 0.24362556 0.24391405 0.24417258 0.24409929\n",
      " 0.24347132 0.24388627 0.24386489 0.24417797 0.24334637 0.24416905\n",
      " 0.24420178 0.2440782  0.24338741 0.24403667 0.24343162 0.24407789\n",
      " 0.24344488 0.24328405 0.24366437 0.24377841 0.24408724 0.24422144\n",
      " 0.24385528 0.24394752 0.24407315 0.24399598 0.2435094  0.24417059\n",
      " 0.24350959 0.24354083 0.24405683 0.2435223  0.24401959 0.24379082\n",
      " 0.2434787  0.24355171 0.24332596 0.2437867  0.24361155 0.24339811\n",
      " 0.24347769 0.24420437 0.24401693 0.24389297 0.24345546 0.24334912\n",
      " 0.24335274 0.24397654 0.24372992 0.24384202 0.24351527 0.24399549\n",
      " 0.2433353  0.24388062 0.24351689 0.2438201 ]\n",
      "weights [[-0.59793007 -0.81091269 -0.27369172 ...  0.5182098   1.0524413\n",
      "   1.12651011]\n",
      " [ 0.34450949  0.95311894 -1.17385108 ...  0.37351934 -0.15033757\n",
      "   1.19247717]\n",
      " [-0.15884575  0.95182964  0.64146937 ... -1.16267604 -0.86609769\n",
      "   0.1410598 ]\n",
      " ...\n",
      " [-0.80638285  0.28264795  0.99394812 ...  0.86721965  0.291471\n",
      "  -1.21552932]\n",
      " [-0.90591005 -0.42517602  0.35464138 ... -0.87639762  1.12747684\n",
      "   0.78313249]\n",
      " [ 0.0045503   0.90842919  0.03623162 ...  0.80779036 -1.22155798\n",
      "   0.98924973]]\n",
      "delta [0.24422102 0.24343578 0.24392697 0.24350237 0.24384492 0.24334213\n",
      " 0.24391293 0.24334608 0.24334231 0.24341844 0.24373281 0.24345932\n",
      " 0.24339405 0.24390948 0.24371653 0.24403863 0.24401004 0.24423316\n",
      " 0.24358916 0.24350295 0.243364   0.24420097 0.24354119 0.24422807\n",
      " 0.24339781 0.24360016 0.24414504 0.24418392 0.24406094 0.24339365\n",
      " 0.24329724 0.24336579 0.24378104 0.24374805 0.24338903 0.24402291\n",
      " 0.24328976 0.24352151 0.24349825 0.24386744 0.24420559 0.2440817\n",
      " 0.24421115 0.24401052 0.24362556 0.24391405 0.24417258 0.24409929\n",
      " 0.24347132 0.24388627 0.24386489 0.24417797 0.24334637 0.24416905\n",
      " 0.24420178 0.2440782  0.24338741 0.24403667 0.24343162 0.24407789\n",
      " 0.24344488 0.24328405 0.24366437 0.24377841 0.24408724 0.24422144\n",
      " 0.24385528 0.24394752 0.24407315 0.24399598 0.2435094  0.24417059\n",
      " 0.24350959 0.24354083 0.24405683 0.2435223  0.24401959 0.24379082\n",
      " 0.2434787  0.24355171 0.24332596 0.2437867  0.24361155 0.24339811\n",
      " 0.24347769 0.24420437 0.24401693 0.24389297 0.24345546 0.24334912\n",
      " 0.24335274 0.24397654 0.24372992 0.24384202 0.24351527 0.24399549\n",
      " 0.2433353  0.24388062 0.24351689 0.2438201 ]\n",
      "weights [[ 1.18592134]\n",
      " [-0.83402382]\n",
      " [ 0.37808435]\n",
      " [-0.67798344]\n",
      " [ 0.16490616]\n",
      " [-1.04969044]\n",
      " [ 0.34125599]\n",
      " [-1.04069313]\n",
      " [-1.04928802]\n",
      " [-0.87429531]\n",
      " [-0.1189191 ]\n",
      " [-0.77912667]\n",
      " [-0.93066076]\n",
      " [ 0.33221339]\n",
      " [-0.1594499 ]\n",
      " [ 0.67627585]\n",
      " [ 0.59898714]\n",
      " [ 1.22091484]\n",
      " [-0.47100566]\n",
      " [-0.67660466]\n",
      " [-0.99972735]\n",
      " [ 1.12841155]\n",
      " [-0.58591097]\n",
      " [ 1.20622513]\n",
      " [-0.92200968]\n",
      " [-0.44447674]\n",
      " [ 0.96998815]\n",
      " [ 1.07980632]\n",
      " [ 0.73706811]\n",
      " [-0.93159977]\n",
      " [-1.15156039]\n",
      " [-0.99563136]\n",
      " [ 0.00215956]\n",
      " [-0.08080126]\n",
      " [-0.94222644]\n",
      " [ 0.63371705]\n",
      " [-1.16844841]\n",
      " [-0.6326797 ]\n",
      " [-0.68770807]\n",
      " [ 0.22293319]\n",
      " [ 1.14161138]\n",
      " [ 0.793977  ]\n",
      " [ 1.15754657]\n",
      " [ 0.60029024]\n",
      " [-0.38295752]\n",
      " [ 0.34417505]\n",
      " [ 1.04761741]\n",
      " [ 0.84251198]\n",
      " [-0.7510292 ]\n",
      " [ 0.2717272 ]\n",
      " [ 0.21633547]\n",
      " [ 1.06290915]\n",
      " [-1.04003436]\n",
      " [ 1.0376404 ]\n",
      " [ 1.13070622]\n",
      " [ 0.78435566]\n",
      " [-0.9459764 ]\n",
      " [ 0.67096951]\n",
      " [-0.84371924]\n",
      " [ 0.78350271]\n",
      " [-0.8128538 ]\n",
      " [-1.18132845]\n",
      " [-0.28821428]\n",
      " [-0.00448137]\n",
      " [ 0.80925047]\n",
      " [ 1.18710713]\n",
      " [ 0.1915507 ]\n",
      " [ 0.43223389]\n",
      " [ 0.77050823]\n",
      " [ 0.56122236]\n",
      " [-0.66137709]\n",
      " [ 1.04198524]\n",
      " [-0.66092513]\n",
      " [-0.58677775]\n",
      " [ 0.72582218]\n",
      " [-0.63082736]\n",
      " [ 0.62473928]\n",
      " [ 0.02690165]\n",
      " [-0.73372343]\n",
      " [-0.56082759]\n",
      " [-1.08650618]\n",
      " [ 0.01648062]\n",
      " [-0.41693018]\n",
      " [-0.9213135 ]\n",
      " [-0.73608448]\n",
      " [ 1.13812924]\n",
      " [ 0.61755043]\n",
      " [ 0.28914461]\n",
      " [-0.78815886]\n",
      " [-1.03374486]\n",
      " [-1.02547549]\n",
      " [ 0.50925608]\n",
      " [-0.12611517]\n",
      " [ 0.15744986]\n",
      " [-0.64747505]\n",
      " [ 0.55990876]\n",
      " [-1.06526577]\n",
      " [ 0.25706133]\n",
      " [-0.64364519]\n",
      " [ 0.10134065]]\n",
      "delta [ 0.87035523  0.03601334  0.31992193  0.30827381 -0.16805956  0.04109513\n",
      " -0.28811292  0.14991103  0.38701383  0.16305354 -0.51673836 -0.04312591\n",
      " -0.06120488 -0.07071367  0.99976274 -0.17586792 -0.1865358  -0.30766423\n",
      "  0.05652439 -0.04554595 -0.03733538 -0.30673258 -0.35331334  0.1694144\n",
      "  0.08969551 -0.18344246  0.85047536  0.07504135  0.10878089 -0.27690198\n",
      " -0.24762126  0.34183039 -0.41834103 -0.4080268   0.08216482 -0.44663393\n",
      "  0.12079018 -0.04341289  0.27023681  0.1562659  -1.11093994  0.25205245\n",
      " -0.89937183 -0.41365266  0.54121812  0.78665518  0.16527457 -0.3632957\n",
      " -0.35682328 -0.32600311 -0.75339503  0.01001193  0.78288638 -0.02867013\n",
      " -0.35148768 -1.19577869 -0.20287981 -0.1594575   0.43565001  0.39309636\n",
      " -0.4330663   0.69252888  0.4341832  -0.31382723  0.221414   -0.4597703\n",
      "  0.3633177   0.278232    0.02313048 -0.47857555 -0.61847376 -0.5123779\n",
      "  0.03513093 -0.1985924   0.35237293  0.82091859 -0.20104349 -0.21770226\n",
      "  0.62472507  0.13147791  0.63309906 -0.04652845  0.42934196 -0.6107324\n",
      "  0.09266725  1.06027101  0.58513716 -0.5727791  -0.11341476  0.22404331\n",
      " -1.03678768  0.42946221 -0.13411914  0.33575624  0.38184593 -0.40997618\n",
      "  0.01592981 -0.24972969  0.18505649  0.0079607 ]\n",
      "weights [[-0.19956339  0.66160495 -0.46711361 -0.0341664   0.53689485 -0.02273223\n",
      "   0.67340737 -1.08034956 -1.01783357 -0.1767048   0.68638136 -0.72219981\n",
      "   0.98834239  0.96969926  0.60047432  0.48276167 -0.88119958 -0.57356354\n",
      "  -0.18739896  0.58222979 -0.98357376 -0.96099852  0.38711257  1.03014441\n",
      "   0.31853432  0.34465118 -0.3594594   0.27773957 -0.96357535 -0.15665491\n",
      "   0.79928608 -0.32773077 -0.89625156 -0.28735874  0.63853389 -0.02977175\n",
      "   0.53484119 -1.03014527 -0.90636198 -0.12665507  0.32750466 -0.53873781\n",
      "   0.23595149 -0.77035505  0.16765208  1.03215511 -0.81259668 -1.01288482\n",
      "  -1.19244041 -0.55838413 -0.23885632  0.59096289 -1.12552814 -0.51541296\n",
      "  -0.72140376  1.11369207 -1.14633247  0.71950882 -0.51767029  0.88155915\n",
      "   0.1926493  -1.0016824  -0.29367818  0.83224574  1.08687472 -0.39572056\n",
      "  -0.45091145  0.39416005 -0.60687258 -1.1379137  -0.45907716 -0.13682394\n",
      "   0.64706909 -0.47609265 -0.46464342  0.75216209  0.48705261  1.19850421\n",
      "  -0.06580333 -1.06854249 -0.28666626 -0.92454726 -1.00897622  0.49279564\n",
      "   0.04540637  0.1302042   1.10872704 -0.42558161  0.95732295  0.309488\n",
      "   0.36643527 -0.3833483  -0.94123843  0.92753341 -1.16305198  0.75475583\n",
      "  -0.05303406 -0.82246934  0.90515427  0.60121252]]\n",
      "delta [-0.08829024]\n",
      "weights [[ 0.88918517  0.7529813 ]\n",
      " [-1.18358761 -0.35361677]\n",
      " [ 0.48814621  0.34862676]\n",
      " [-1.12763691  0.87514984]\n",
      " [ 0.03312431 -0.41060865]\n",
      " [ 1.20437353 -0.42796837]\n",
      " [-0.94409098 -0.8338671 ]\n",
      " [ 0.76571402 -0.81278388]\n",
      " [-0.69748868  0.4435001 ]\n",
      " [ 0.46093014 -0.01184117]\n",
      " [-0.94454329 -0.54565466]\n",
      " [ 0.82546429 -1.08738877]\n",
      " [ 0.82752045 -1.1616428 ]\n",
      " [ 0.36886484  0.68562169]\n",
      " [ 0.03534545 -1.20076558]\n",
      " [-0.88255344  0.72773507]\n",
      " [-0.83595497  0.86878816]\n",
      " [ 0.286413   -0.67671365]\n",
      " [ 1.06455467 -0.28245304]\n",
      " [-0.60143829  0.19149174]\n",
      " [-0.58095865  0.7995102 ]\n",
      " [-0.92525389  0.03039572]\n",
      " [-0.0349489   0.35840223]\n",
      " [-0.78777349 -0.46301045]\n",
      " [ 0.899586    0.63653229]\n",
      " [-0.07293927  0.72613193]\n",
      " [-0.58922645  0.36119019]\n",
      " [-0.92218855  0.69549662]\n",
      " [-0.23429321 -0.47406012]\n",
      " [ 0.39882358  0.15865993]\n",
      " [ 0.97676081 -0.09173417]\n",
      " [ 1.10462115 -0.53079126]\n",
      " [-0.09048204  0.64167703]\n",
      " [-0.41643131 -0.98420496]\n",
      " [ 0.83246461 -0.79347046]\n",
      " [ 0.45752323  0.29857247]\n",
      " [ 0.00817431 -1.09499187]\n",
      " [ 0.42812281  0.22323301]\n",
      " [-0.59973814  0.45732191]\n",
      " [ 0.41759941  0.71624426]\n",
      " [ 0.69776067  0.26602087]\n",
      " [-0.26874972 -0.72803902]\n",
      " [ 0.54633771  0.44983825]\n",
      " [-0.28582541 -0.48205352]\n",
      " [ 0.56641369  0.06807267]\n",
      " [ 0.09768677 -0.9228971 ]\n",
      " [ 0.86031425  0.72784105]\n",
      " [-1.13361446 -0.17889253]\n",
      " [ 0.71124455 -0.3847053 ]\n",
      " [-0.43978369 -0.8915014 ]\n",
      " [ 0.33082363  0.39887967]\n",
      " [ 0.72234266  0.99464222]\n",
      " [-0.20774679  0.93636131]\n",
      " [-1.17147528 -0.0867075 ]\n",
      " [-0.38091442 -0.72399276]\n",
      " [-0.32811812  0.17442542]\n",
      " [ 1.15535471 -1.06062036]\n",
      " [-0.19948978  0.94888194]\n",
      " [ 0.36528917  0.89167654]\n",
      " [-0.08658885  0.70330196]\n",
      " [-0.79470985 -0.46760595]\n",
      " [ 0.13388555  0.31694424]\n",
      " [-0.43361389 -1.1458022 ]\n",
      " [ 0.09330493  0.06274863]\n",
      " [-1.1282588  -0.33111044]\n",
      " [ 0.62735567  0.23371318]\n",
      " [ 0.1006751   1.17448241]\n",
      " [-0.22992043 -1.01960292]\n",
      " [ 0.41915697 -0.84476378]\n",
      " [-0.89521854 -0.09313599]\n",
      " [-1.09870062  0.09053011]\n",
      " [ 0.45893224  0.90522855]\n",
      " [ 0.15176134 -0.03825219]\n",
      " [ 1.22406238 -0.9846789 ]\n",
      " [-1.16189326  0.25172751]\n",
      " [ 0.22319523  0.4876761 ]\n",
      " [-1.09711775 -1.21593252]\n",
      " [ 0.77189774 -0.49638325]\n",
      " [ 0.75003693  0.27424881]\n",
      " [ 1.13833023  1.13772596]\n",
      " [ 0.45804797 -0.9136695 ]\n",
      " [-0.18143818  0.53200714]\n",
      " [-0.34501112  0.68099398]\n",
      " [ 0.6864097   0.6866205 ]\n",
      " [-0.65283069 -0.30123314]\n",
      " [ 0.26360037  1.22447646]\n",
      " [ 0.70522425 -0.38011052]\n",
      " [ 0.87390239 -0.75414846]\n",
      " [-0.26502735 -0.24516415]\n",
      " [-0.25248763 -0.65730215]\n",
      " [-0.45472565  0.58713453]\n",
      " [ 0.98823375  0.44735652]\n",
      " [ 0.04082072  0.06131948]\n",
      " [ 0.27532844  0.00400372]\n",
      " [ 0.77368802 -0.94144657]\n",
      " [ 0.60936697 -0.48487719]\n",
      " [ 1.06108823  0.4134934 ]\n",
      " [ 0.16457128  0.50969859]\n",
      " [ 0.9154258   0.67368412]\n",
      " [-0.88270125  0.80388826]]\n",
      "delta [ 0.00432671 -0.01246453  0.01030931  0.00074062 -0.01120618  0.0004993\n",
      " -0.01200983  0.02276943  0.02187739  0.00386849 -0.01287396  0.01487001\n",
      " -0.02018325 -0.02136286 -0.01130035 -0.01047734  0.01928147  0.0118596\n",
      "  0.00412164 -0.01233745  0.02160129  0.01958748 -0.00848373 -0.0199086\n",
      " -0.00694517 -0.0076062   0.00772216 -0.00600716  0.01957476  0.00344385\n",
      " -0.01762404  0.00715239  0.01976146  0.00532314 -0.01353465  0.00065672\n",
      " -0.01020577  0.02269077  0.01958459  0.00278643 -0.00722881  0.01055349\n",
      " -0.00520603  0.0155671  -0.00368798 -0.02038317  0.01766109  0.01970757\n",
      "  0.02585986  0.01047689  0.00526948 -0.01271237  0.02483932  0.01012371\n",
      "  0.01398238 -0.02391388  0.0242049  -0.01587708  0.01134052 -0.01945121\n",
      " -0.00371792  0.02200566  0.00527631 -0.01806983 -0.02067602  0.00873279\n",
      "  0.00983729 -0.00741565  0.01244336  0.02290548  0.00929076  0.00298798\n",
      " -0.01400087  0.01015608  0.00953406 -0.01659584 -0.00786844 -0.02588424\n",
      "  0.00145223  0.02214984  0.00584951  0.02030972  0.0221866  -0.010789\n",
      " -0.00090924 -0.00281634 -0.02404501  0.00907086 -0.01990105 -0.00613203\n",
      " -0.00801186  0.00840245  0.02039344 -0.02020766  0.02422643 -0.01619176\n",
      "  0.00116174  0.01814408 -0.01968921 -0.01309925]\n",
      "weights [[-0.93995635 -0.91142893  0.72348732  0.84031349]\n",
      " [-0.32713922  0.54205253  1.16721515 -0.11440623]]\n",
      "delta [0.00328425 0.01555493]\n",
      "weights [[-8.80956460e-04 -1.24601050e-01]\n",
      " [-2.89710608e-01 -4.61663718e-01]\n",
      " [ 7.63504782e-01 -4.29636878e-03]\n",
      " [-1.01737998e+00  1.74541992e-01]]\n",
      "delta [-0.00199528  0.00124804  0.00512032  0.00024031]\n",
      "weights [[-0.93464389  0.65889126  0.87676519 -1.01868711]\n",
      " [ 0.59091807  0.93344216  0.68034723  1.12936284]]\n",
      "delta [ 4.65551223e-04 -6.69230210e-05]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9826926579181234]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input and output layers must be with equal numbers\n",
    "\n",
    "layers = [\n",
    "    Linear(4,13, activation=Sigmoid()),\n",
    "    Linear(13,4, activation=Sigmoid()),\n",
    "    Linear(4,2, activation=Sigmoid()),\n",
    "    Linear(2,4, activation=Sigmoid()),\n",
    "    Linear(4,2, activation=Sigmoid()),\n",
    "    Linear(2,100, activation=Sigmoid()),\n",
    "    Linear(100,1, activation=Sigmoid()),\n",
    "    Linear(1,100, activation=Sigmoid()),\n",
    "    Linear(100,100, activation=Softmax()),\n",
    "]\n",
    "\n",
    "model = Model(layers=layers, loss=MSELoss(), n_epoch=1, verbose=0)\n",
    "\n",
    "\n",
    "model.fit(dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.87616528,  0.8770434 , -1.07185091, -0.3473528 ,  0.07240868,\n",
       "        -0.0397271 ,  0.04991074, -1.00090593,  0.99031344,  1.10922691,\n",
       "         0.56103235, -0.71546477,  1.19246648],\n",
       "       [ 0.29093634,  0.58343993,  0.06555826,  0.26530925, -0.87390265,\n",
       "        -0.76867888, -0.39083652, -0.94488054,  0.90105738,  0.65674802,\n",
       "         0.83505957, -0.72995415, -1.15744424],\n",
       "       [ 0.59840803,  0.97942934, -0.92458376, -1.03876575, -0.6212136 ,\n",
       "        -0.08884953,  0.68199734,  0.25303518, -0.47230019,  1.11874919,\n",
       "         0.13748123,  0.1428907 ,  0.72724899],\n",
       "       [-0.09438416, -0.39631527,  0.96266206,  0.61768845, -0.42897597,\n",
       "         1.05387946,  0.50110398,  1.00928054, -0.98790323,  0.02746748,\n",
       "         0.95923375, -0.06502083, -0.64327335]])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9391043  0.00322892 0.00930119 0.94782751 0.91798714 0.01339995\n",
      " 0.00372608 0.00276383 0.03399139 0.96232475 0.86889642 0.67765143\n",
      " 0.00543836]\n",
      "\n",
      "[0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (13,) and (4,) not aligned: 13 (dim 0) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[369], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[366], line 58\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28mprint\u001b[39m(D[i])\n\u001b[0;32m---> 58\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\n\u001b[1;32m     61\u001b[0m mean_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(epoch_losses)\n\u001b[1;32m     62\u001b[0m losses_by_epoch\u001b[38;5;241m.\u001b[39mappend(mean_loss)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (13,) and (4,) not aligned: 13 (dim 0) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "model.fit(dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 970,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset['data'], dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 1091,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0), np.int64(1), np.int64(2)}"
      ]
     },
     "execution_count": 1093,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = set(y)\n",
    "labels_len = len(labels)\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1]]"
      ]
     },
     "execution_count": 1094,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1 = []\n",
    "\n",
    "for i in y:\n",
    "    l = [0] * labels_len\n",
    "    l[i] = 1\n",
    "    y_1.append(l)\n",
    "y_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y_1, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1229,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(data=X_train, targets=y_train)\n",
    "val_dataset = Dataset(data=X_val, targets=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1267,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Linear(4,10, activation=ReLU()),\n",
    "    Linear(10,20, activation=ReLU()),\n",
    "    Linear(20,100, activation=ReLU()),\n",
    "    Linear(100,10, activation=ReLU()),\n",
    "    Linear(10,3, activation=Softmax()),\n",
    "]\n",
    "\n",
    "model = Model(layers=layers, loss=MSELoss(), n_epoch=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs: 100%|██████████| 10/10 [00:00<00:00, 41.55it/s]\n"
     ]
    }
   ],
   "source": [
    "losses = model.fit(dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 1269,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.3, 3. , 1.1, 0.1])"
      ]
     },
     "execution_count": 1240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.37434921, 0.37134219, 0.2543086 ]]), [1, 0, 0])"
      ]
     },
     "execution_count": 1241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[1]), y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.23116393, 0.37761343, 0.39122264]]), [0, 1, 0])"
      ]
     },
     "execution_count": 1242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_val[1]), y_val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3416666666666667"
      ]
     },
     "execution_count": 1270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_count = 0\n",
    "all_count = 0\n",
    "\n",
    "for sample,target in zip(X_train, y_train):\n",
    "    pred = model.predict(sample=sample)\n",
    "\n",
    "    pred = np.where(pred[0] >= max(pred[0]), 1, 0)\n",
    "    # print(pred)\n",
    "    # print(target)\n",
    "\n",
    "    if list(pred) == target:\n",
    "        true_count += 1\n",
    "    all_count += 1 \n",
    "\n",
    "\n",
    "acc = true_count / all_count\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 1215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([1,0,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
