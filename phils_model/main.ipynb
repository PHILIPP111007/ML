{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, TypeAlias\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample: TypeAlias = list[int | float]\n",
    "Data: TypeAlias = list[Sample]\n",
    "\n",
    "Target: TypeAlias = int | float\n",
    "Targets: TypeAlias = list[Target]\n",
    "\n",
    "Weights: TypeAlias = list[list[float]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class ActivationBase(ABC):\n",
    "    @abstractmethod\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        \"\"\"Apply the activation function to an layer output\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, x: Sample):\n",
    "        pass\n",
    "\n",
    "#######################################\n",
    "\n",
    "\n",
    "class ReLU(ActivationBase):\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def derivative(self, x: Sample):\n",
    "        return self.calc(x=x)\n",
    "\n",
    "\n",
    "class Sigmoid(ActivationBase):\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x: Sample):\n",
    "        return x * (1-x)\n",
    "\n",
    "\n",
    "class Softmax(ActivationBase):\n",
    "    \"\"\"returns model 'probabilities' for each class\"\"\"\n",
    "\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        \n",
    "        # optimization: make numbers in an array from -inf to 0 because of a np.exp growing\n",
    "        # and returns an array of floats from 0.0 to 1.0\n",
    "        max_value = np.max(x)\n",
    "        x -= max_value\n",
    "\n",
    "        exp_values = np.exp(x)\n",
    "        return exp_values / np.sum(exp_values)\n",
    "    \n",
    "    def derivative(self, x: Sample):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "\n",
    "class LossBase(ABC):\n",
    "    @abstractmethod\n",
    "    def calc(self, x: Sample, y: Target) -> float:\n",
    "        \"\"\"Apply the loss function to an output layer\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class MSELoss(LossBase):\n",
    "    \"\"\"For regression\"\"\" \n",
    "    def calc(self, x: Sample, y: Target) -> float:\n",
    "\n",
    "        loss = (y - x) ** 2\n",
    "\n",
    "        return np.mean(loss)\n",
    "\n",
    "\n",
    "class CrossEntropy(LossBase):\n",
    "    \"\"\"For classification\"\"\"\n",
    "    def calc(self, x: Sample, y: Target) -> float:\n",
    "        return -np.sum(y * np.log(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.39262147e-44 7.31058579e-01 9.74950551e-35 2.68941421e-01]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = [1, 100, 22, 99]\n",
    "\n",
    "f = Softmax()\n",
    "\n",
    "b = f.calc(a)\n",
    "print(b)\n",
    "print(sum(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data: Data, targets: Targets) -> None:\n",
    "        self.data: Data = data\n",
    "        self._len = len(data)\n",
    "        self.targets: Targets = targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._len\n",
    "    \n",
    "    def __getitem__(self, index) -> Sample:\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_inputs: int, n_neurons: int, activation: ActivationBase) -> None:\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        \n",
    "        self.weights = self._init_weights()\n",
    "        self.bias = self._init_bias()\n",
    "        self.output = []\n",
    "\n",
    "        self.activation = activation\n",
    "    \n",
    "    def _init_weights(self) -> list[float]:\n",
    "        scale = 1/max(1., (2+2)/2.)\n",
    "        limit = np.sqrt(3.0 * scale)\n",
    "\n",
    "        return np.random.randn(self.n_neurons, self.n_inputs) * 0.1\n",
    "    \n",
    "        # weights = np.random.uniform(-limit, limit, size=(self.n_neurons, self.n_inputs))\n",
    "        # return weights\n",
    "    \n",
    "    def _init_bias(self) -> list[float]:\n",
    "        return np.random.randn(1)\n",
    "    \n",
    "    def forward(self, inputs) -> None:\n",
    "        output = np.matmul( self.weights, inputs.T)\n",
    "        output += self.bias\n",
    "        self.output = self.activation.calc(output)\n",
    "\n",
    "\n",
    "Layers: TypeAlias = list[Linear]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers: Layers, loss: LossBase):\n",
    "        self.layers = layers\n",
    "        self._layers_len = len(layers)\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, dataset: Dataset, n_epoch: int = 1, learning_rate: float = 0.01, verbose: bool = True) -> list[float]:\n",
    "        losses_by_epoch = []\n",
    "\n",
    "        range_epoch = range(n_epoch)\n",
    "        if verbose:\n",
    "            range_epoch = tqdm(range_epoch, desc=\"epochs\", position=0)\n",
    "\n",
    "        for epoch in range_epoch:\n",
    "            epoch_losses = []\n",
    "\n",
    "            for i,sample in enumerate(dataset):\n",
    "                sample = np.array(sample)\n",
    "\n",
    "                # Forward pass\n",
    "                self.layers[0].forward(inputs=sample) # input layer\n",
    "                for j in range(1, self._layers_len):\n",
    "                    self.layers[j].forward(inputs=self.layers[j-1].output)\n",
    "\n",
    "                target = dataset.targets[i]\n",
    "\n",
    "\n",
    "                # Calc loss\n",
    "                output_error = self.calc_loss(target=target)\n",
    "                epoch_losses.append(output_error)\n",
    "\n",
    "\n",
    "                # Backward pass\n",
    "                delta =  self.layers[-2].activation.derivative(x=self.layers[-1].output) * output_error\n",
    "\n",
    "                for i in range(self._layers_len - 2, -1, -1):\n",
    "                    error = np.dot(self.layers[i+1].weights.T, delta)\n",
    "                    delta =  self.layers[i].activation.derivative(x=self.layers[i].output) * error\n",
    "\n",
    "                    self.layers[i+1].weights += np.dot(delta, self.layers[i].output.T) * learning_rate\n",
    "\n",
    "            mean_loss = np.mean(epoch_losses)\n",
    "            losses_by_epoch.append(mean_loss)\n",
    "        \n",
    "        return losses_by_epoch\n",
    "    \n",
    "    def predict(self, sample: Sample) -> list[float]:\n",
    "        sample = np.array(sample)\n",
    "        # sample = sample.reshape(1,len(sample))\n",
    "\n",
    "        self.layers[0].forward(inputs=sample)\n",
    "                \n",
    "        for j in range(1, self._layers_len):\n",
    "            self.layers[j].forward(inputs=self.layers[j-1].output)\n",
    "        \n",
    "        predict = self.layers[-1].output\n",
    "        return predict\n",
    "    \n",
    "    def calc_loss(self, target: Target) -> float:\n",
    "        output_layer = self.layers[-1]\n",
    "        output = output_layer.output\n",
    "\n",
    "        loss = self.loss.calc(x=output, y=target)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def set_weights(self, weights: Weights) -> None:\n",
    "        for w,layer in zip(weights, self.layers):\n",
    "            layer.weights = w\n",
    "\n",
    "    @property\n",
    "    def weights(self) -> Weights:\n",
    "        weights = [layer.weights for layer in self.layers]\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\n",
    "    # [1,2,3,4],\n",
    "    [4,3,2,1]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# X_train = [\n",
    "#     [[1,2,3], [1,2,3], [1,2,3]], # photo\n",
    "# ]\n",
    "\n",
    "y_train = [1,]\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = Dataset(data=X_train, targets=y_train)\n",
    "# val_dataset = Dataset(data=X_val, targets=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6666666666666665]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input and output layers must be with equal numbers\n",
    "\n",
    "layers = [\n",
    "    Linear(4,13, activation=Sigmoid()),\n",
    "    Linear(13,4, activation=Sigmoid()),\n",
    "    Linear(4,2, activation=Sigmoid()),\n",
    "    Linear(2,4, activation=Sigmoid()),\n",
    "    Linear(4,2, activation=Sigmoid()),\n",
    "    Linear(2,100, activation=Sigmoid()),\n",
    "    Linear(100,1, activation=Sigmoid()),\n",
    "    Linear(1,1, activation=Sigmoid()),\n",
    "    Linear(1,1, activation=Softmax()),\n",
    "]\n",
    "\n",
    "model = Model(layers=layers, loss=MSELoss())\n",
    "\n",
    "\n",
    "model.fit(dataset=train_dataset, n_epoch=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0364703 ,  0.28592891,  0.05205459, -0.23562025, -0.13169687,\n",
       "         0.05773848,  0.15587046, -0.07733469, -0.17965607, -0.00313857,\n",
       "        -0.09769579, -0.03839707,  0.06037566],\n",
       "       [-0.00444996,  0.13219194, -0.01261153,  0.11913717,  0.04289459,\n",
       "         0.04846385, -0.00633721, -0.02220829, -0.0693783 ,  0.09911889,\n",
       "        -0.1472693 , -0.05517775,  0.08246574],\n",
       "       [ 0.0108675 ,  0.26054019, -0.09978563,  0.15491572, -0.07706601,\n",
       "        -0.11919199,  0.13310621,  0.17860847, -0.04446456, -0.11231901,\n",
       "        -0.10296507, -0.07867678, -0.08930322],\n",
       "       [-0.14322035,  0.01618757, -0.12069708,  0.00693557, -0.01779432,\n",
       "        -0.02126548,  0.05125332,  0.14137431, -0.08267617, -0.08961813,\n",
       "         0.0277973 ,  0.04295598, -0.17798387]])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs: 100%|██████████| 1/1 [00:00<00:00, 15.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6666666666666665]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset['data'], dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = set(y)\n",
    "labels_len = len(labels)\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1]]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1 = []\n",
    "\n",
    "for i in y:\n",
    "    l = [0] * labels_len\n",
    "    l[i] = 1\n",
    "    y_1.append(l)\n",
    "y_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y_1, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(data=X_train, targets=y_train)\n",
    "val_dataset = Dataset(data=X_val, targets=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Linear(4,4, activation=ReLU()),\n",
    "    Linear(4,3, activation=ReLU()),\n",
    "    Linear(3,3, activation=ReLU()),\n",
    "]\n",
    "\n",
    "model = Model(layers=layers, loss=MSELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs: 100%|██████████| 100/100 [00:00<00:00, 158.58it/s]\n"
     ]
    }
   ],
   "source": [
    "losses = model.fit(dataset=train_dataset, n_epoch=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468,\n",
       " 0.22795295322387468]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phil/micromamba/envs/bio/lib/python3.12/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/Users/phil/micromamba/envs/bio/lib/python3.12/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.4, 2.8, 5.6, 2.2])"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.25763174, 0.25763174, 0.25763174]), [1, 0, 0])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[1]), y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.25763174, 0.25763174, 0.25763174]), [0, 0, 1])"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_val[1]), y_val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[392], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample,target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_train, y_train):\n\u001b[1;32m      5\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(sample\u001b[38;5;241m=\u001b[39msample)\n\u001b[0;32m----> 7\u001b[0m     pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(pred[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# print(pred)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print(target)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(pred) \u001b[38;5;241m==\u001b[39m target:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "true_count = 0\n",
    "all_count = 0\n",
    "\n",
    "for sample,target in zip(X_train, y_train):\n",
    "    pred = model.predict(sample=sample)\n",
    "\n",
    "    pred = np.where(pred[0] >= max(pred[0]), 1, 0)\n",
    "    # print(pred)\n",
    "    # print(target)\n",
    "\n",
    "    if list(pred) == target:\n",
    "        true_count += 1\n",
    "    all_count += 1 \n",
    "\n",
    "\n",
    "acc = true_count / all_count\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([1,0,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
