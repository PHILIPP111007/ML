{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, TypeAlias\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample: TypeAlias = list[int | float]\n",
    "Data: TypeAlias = list[Sample]\n",
    "\n",
    "Target: TypeAlias = int | float\n",
    "Targets: TypeAlias = list[Target]\n",
    "\n",
    "Weights: TypeAlias = list[list[float]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class ActivationBase(ABC):\n",
    "    @abstractmethod\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        \"\"\"Apply the activation function to an layer output\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, x: Sample):\n",
    "        pass\n",
    "\n",
    "#######################################\n",
    "\n",
    "\n",
    "class ReLU(ActivationBase):\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def derivative(self, x: Sample):\n",
    "        return self.calc(x=x)\n",
    "\n",
    "\n",
    "class Sigmoid(ActivationBase):\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x: Sample):\n",
    "        return x * (1-x)\n",
    "\n",
    "\n",
    "class Softmax(ActivationBase):\n",
    "    \"\"\"returns model 'probabilities' for each class\"\"\"\n",
    "\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        \n",
    "        # optimization: make numbers in an array from -inf to 0 because of a np.exp growing\n",
    "        # and returns an array of floats from 0.0 to 1.0\n",
    "        max_value = np.max(x)\n",
    "        x -= max_value\n",
    "\n",
    "        exp_values = np.exp(x)\n",
    "        return exp_values / np.sum(exp_values)\n",
    "    \n",
    "    def derivative(self, x: Sample):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "\n",
    "class LossBase(ABC):\n",
    "    @abstractmethod\n",
    "    def calc(self, x: Sample, y: Target) -> float:\n",
    "        \"\"\"Apply the loss function to an output layer\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class MSELoss(LossBase):\n",
    "    \"\"\"For regression\"\"\" \n",
    "    def calc(self, x: Sample, y: Target) -> float:\n",
    "\n",
    "        x = np.argmax(x) + 1\n",
    "        y = np.argmax(y)  +1\n",
    "\n",
    "        loss = (y - x) ** 2\n",
    "\n",
    "        return np.mean(loss)\n",
    "\n",
    "\n",
    "class CrossEntropy(LossBase):\n",
    "    \"\"\"For classification\"\"\"\n",
    "    def calc(self, x: Sample, y: Target) -> float:\n",
    "        return -np.sum(y * np.log(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.39262147e-44 7.31058579e-01 9.74950551e-35 2.68941421e-01]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = [1, 100, 22, 99]\n",
    "\n",
    "f = Softmax()\n",
    "\n",
    "b = f.calc(a)\n",
    "print(b)\n",
    "print(sum(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data: Data, targets: Targets) -> None:\n",
    "        self.data: Data = data\n",
    "        self._len = len(data)\n",
    "        self.targets: Targets = targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._len\n",
    "    \n",
    "    def __getitem__(self, index) -> Sample:\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_inputs: int, n_neurons: int, activation: ActivationBase) -> None:\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        \n",
    "        self.weights = self._init_weights()\n",
    "        self.bias = self._init_bias()\n",
    "        self.output = []\n",
    "\n",
    "        self.activation = activation\n",
    "    \n",
    "    def _init_weights(self) -> list[float]:\n",
    "        weights = np.random.randn(self.n_neurons, self.n_inputs) * 0.1\n",
    "        return weights\n",
    "    \n",
    "    def _init_bias(self) -> list[float]:\n",
    "        return np.random.randn(1)\n",
    "    \n",
    "    def forward(self, inputs) -> None:\n",
    "        output = np.dot( self.weights, inputs)\n",
    "        output += self.bias\n",
    "        self.output = self.activation.calc(output)\n",
    "\n",
    "\n",
    "Layers: TypeAlias = list[Linear]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers: Layers, loss: LossBase):\n",
    "        self.layers = layers\n",
    "        self._layers_len = len(layers)\n",
    "        self.loss = loss\n",
    "    \n",
    "    def fit(self, dataset: Dataset, n_epoch: int = 1, learning_rate: float = 0.01, verbose: bool = True) -> list[float]:\n",
    "        losses_by_epoch = []\n",
    "\n",
    "        range_epoch = range(n_epoch)\n",
    "        if verbose:\n",
    "            range_epoch = tqdm(range_epoch, desc=\"epochs\", position=0)\n",
    "\n",
    "        for epoch in range_epoch:\n",
    "            epoch_losses = []\n",
    "\n",
    "            for i,sample in enumerate(dataset):\n",
    "                sample = np.array(sample)\n",
    "\n",
    "                # Forward pass\n",
    "                self.layers[0].forward(inputs=sample) # input layer\n",
    "                for j in range(1, self._layers_len):\n",
    "                    self.layers[j].forward(inputs=self.layers[j-1].output)\n",
    "\n",
    "                target = dataset.targets[i]\n",
    "\n",
    "\n",
    "                # Calc loss\n",
    "                output_error = self.calc_loss(target=target)\n",
    "                epoch_losses.append(output_error)\n",
    "                # print(output_error)\n",
    "\n",
    "                D = []\n",
    "                # Backward pass\n",
    "                delta =  self.layers[-2].activation.derivative(x=self.layers[-1].output) * output_error\n",
    "                D.append(delta)\n",
    "                for i in range(self._layers_len - 2, -1, -1):\n",
    "                    # error = np.dot(self.layers[i+1].weights.T, delta)\n",
    "                    # delta =  self.layers[i].activation.derivative(x=self.layers[i].output) * error\n",
    "                    # self.layers[i+1].weights += np.dot(delta, self.layers[i].output.T) * learning_rate\n",
    "\n",
    "                    delta = np.dot(D[-1], self.layers[i+1].weights)\n",
    "                    delta *= self.layers[i].activation.derivative(self.layers[i].output)\n",
    "                    D.append(delta)\n",
    "                \n",
    "\n",
    "                D = D[::-1]\n",
    "                for i in range(self._layers_len):\n",
    "                    self.layers[i].weights += -learning_rate * np.dot(self.layers[i].output, D[i])\n",
    "                \n",
    "                print(D)\n",
    "\n",
    "            mean_loss = np.mean(epoch_losses)\n",
    "            losses_by_epoch.append(mean_loss)\n",
    "        \n",
    "        return losses_by_epoch\n",
    "    \n",
    "    def predict(self, sample: Sample) -> list[float]:\n",
    "        sample = np.array(sample)\n",
    "        # sample = sample.reshape(1,len(sample))\n",
    "\n",
    "        self.layers[0].forward(inputs=sample)\n",
    "                \n",
    "        for i in range(1, self._layers_len):\n",
    "            self.layers[i].forward(inputs=self.layers[i-1].output)\n",
    "        \n",
    "        predict = self.layers[-1].output\n",
    "        return predict\n",
    "    \n",
    "    def calc_loss(self, target: Target) -> float:\n",
    "        output_layer = self.layers[-1]\n",
    "        output = output_layer.output\n",
    "\n",
    "        # print(output, target)\n",
    "\n",
    "        loss = self.loss.calc(x=output, y=target)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def set_weights(self, weights: Weights) -> None:\n",
    "        for w,layer in zip(weights, self.layers):\n",
    "            layer.weights = w\n",
    "\n",
    "    @property\n",
    "    def weights(self) -> Weights:\n",
    "        weights = [layer.weights for layer in self.layers]\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset['data'], dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{np.int64(0), np.int64(1), np.int64(2)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = set(y)\n",
    "labels_len = len(labels)\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1 = []\n",
    "\n",
    "for i in y:\n",
    "    l = [0] * labels_len\n",
    "    l[i] = 1\n",
    "    y_1.append(l)\n",
    "y_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y_1, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(data=X_train, targets=y_train)\n",
    "val_dataset = Dataset(data=X_val, targets=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Linear(4,3, activation=ReLU()),\n",
    "    Linear(3,3, activation=Softmax()),\n",
    "]\n",
    "\n",
    "model = Model(layers=layers, loss=CrossEntropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.05588644,  0.        , -0.03912095]), array([0.262368  , 0.36553503, 0.26324173])]\n",
      "[array([ 0.05819741,  0.        , -0.04317734]), array([0.25431255, 0.36471491, 0.25521971])]\n",
      "[array([ 0.05000286,  0.        , -0.04836586]), array([0.36210229, 0.46351854, 0.36300873])]\n",
      "[array([ 0.05077463,  0.        , -0.04900364]), array([0.25949705, 0.36525583, 0.26032694])]\n",
      "[array([ 0.04193324,  0.        , -0.05582209]), array([0.36221625, 0.46203547, 0.36238614])]\n",
      "[array([ 0.07659233,  0.        , -0.08932154]), array([0.35743055, 0.53554212, 0.35805712])]\n",
      "[array([ 0.04340049,  0.        , -0.0570885 ]), array([0.26379788, 0.36568276, 0.2650301 ])]\n",
      "[array([ 0.03643865,  0.        , -0.06555303]), array([0.36219591, 0.46216832, 0.3626303 ])]\n",
      "[array([ 0.03793528,  0.        , -0.06380053]), array([0.26425661, 0.36571356, 0.26525747])]\n",
      "[array([ 0.06380337,  0.        , -0.10930437]), array([0.35690933, 0.53328761, 0.35824251])]\n",
      "[array([ 0.03391777,  0.        , -0.07863024]), array([0.36179728, 0.4692634 , 0.36281768])]\n",
      "[array([ 0.0332888, -0.       , -0.0821888]), array([0.36165832, 0.47139815, 0.36317554])]\n",
      "[array([ 0.0516847 , -0.        , -0.12316471]), array([0.35645313, 0.528639  , 0.35857992])]\n",
      "[array([ 0.02383325, -0.        , -0.08931093]), array([0.36200762, 0.4656819 , 0.36258168])]\n",
      "[array([ 0.02558581, -0.        , -0.0863011 ]), array([0.25877141, 0.36521298, 0.26019589])]\n",
      "[array([ 0.04302224, -0.        , -0.15098639]), array([0.35583471, 0.54310021, 0.35765439])]\n",
      "[array([ 0.02393378, -0.        , -0.10235399]), array([0.24364997, 0.36342796, 0.24489078])]\n",
      "[array([ 0.03968671, -0.        , -0.17790928]), array([0.35466347, 0.56308099, 0.35634441])]\n",
      "[array([ 0.01570671, -0.        , -0.11554817]), array([0.36123673, 0.47928638, 0.36271514])]\n",
      "[array([ 0.03086077, -0.        , -0.19276217]), array([0.35383013, 0.56595698, 0.35619864])]\n",
      "[array([ 0.02887956, -0.        , -0.22055494]), array([0.35279206, 0.58782583, 0.35467413])]\n",
      "[array([ 0.00758087, -0.        , -0.14227361]), array([0.3604611 , 0.49305095, 0.36213881])]\n",
      "[array([ 0.01794419, -0.        , -0.23584774]), array([0.35174988, 0.59014177, 0.35457872])]\n",
      "[array([ 0.00067587, -0.        , -0.15747717]), array([0.36015419, 0.49831865, 0.36193655])]\n",
      "[array([ 0.00700599, -0.        , -0.26644793]), array([0.35049942, 0.60519682, 0.35352902])]\n",
      "[array([-0.00215869, -0.        , -0.15227365]), array([0.22162764, 0.3599344 , 0.2237666 ])]\n",
      "[array([-0.00980276, -0.        , -0.18530594]), array([0.35915854, 0.51438725, 0.36178171])]\n",
      "[array([-0.01397993, -0.        , -0.19508154]), array([0.35899429, 0.51724097, 0.36148356])]\n",
      "[array([-0.01120055, -0.        , -0.1734855 ]), array([0.20833607, 0.35715304, 0.21055195])]\n",
      "[array([-0.01428716, -0.        , -0.17381699]), array([0.20879069, 0.35730709, 0.21144403])]\n",
      "[array([-0.02525023, -0.        , -0.22096172]), array([0.358205  , 0.52961648, 0.36114366])]\n",
      "[array([-0.02078823, -0.        , -0.18667979]), array([0.20172669, 0.35562263, 0.20440617])]\n",
      "[array([-0.03524438, -0.        , -0.25855818]), array([0.35677988, 0.55168177, 0.359903  ])]\n",
      "[array([-0.02719589, -0.        , -0.2043848 ]), array([0.18507378, 0.35099141, 0.18785708])]\n",
      "[array([-0.05870185, -0.        , -0.46235193]), array([0.33820532, 0.72694634, 0.34429718])]\n",
      "[array([-0.07426421, -0.        , -0.5578822 ]), array([0.33048239, 0.80692642, 0.3374626 ])]\n",
      "[array([-0.04593859, -0.02644555, -0.2467829 ]), array([0.16449348, 0.34387655, 0.16781708])]\n",
      "[array([-0.11424892, -0.07692677, -0.66838581]), array([0.32407973, 0.85431636, 0.33337442])]\n",
      "[array([-0.06068781, -0.06509687, -0.28319502]), array([0.13891978, 0.33228601, 0.14279605])]\n",
      "[array([-0.06772872, -0.08027256, -0.29544846]), array([0.14461879, 0.33523136, 0.14862429])]\n",
      "[array([-0.07374785, -0.09014082, -0.30425934]), array([0.14803243, 0.33691284, 0.15215057])]\n",
      "[array([-0.15379764, -0.17692764, -0.58009081]), array([0.34783018, 0.67000482, 0.35530865])]\n",
      "[array([-0.23763388, -0.38466373, -0.95387591]), array([0.31690469, 0.90698586, 0.32868355])]\n",
      "[array([-0.33098958, -0.74998552, -1.31200356]), array([0.29692218, 1.0855928 , 0.311346  ])]\n",
      "[array([-0.50745026, -1.84630531, -2.07782394]), array([0.25025182, 1.4888817 , 0.2695019 ])]\n",
      "[array([-0.92050568, -5.61033229, -3.90324224]), array([0.16716845, 2.27797821, 0.18907121])]\n",
      "[array([ -2.27550072, -13.19463833,  -7.26989055]), array([0.11565722, 3.11906897, 0.13817008])]\n",
      "[array([-1.31689400e-05, -6.49170809e-05, -3.18855703e-05]), array([3.05997232e-12, 3.12443689e-06, 6.70217928e-12])]\n",
      "[array([-7.32693414e-05, -3.61075931e-04, -1.77295457e-04]), array([1.37767607e-10, 2.02094476e-05, 2.70666548e-10])]\n",
      "[array([-9.30474132e-07, -4.58572578e-06, -2.25201438e-06]), array([9.19550539e-15, 1.81784594e-07, 2.38501423e-14])]\n",
      "[array([-8.36337749e-07, -4.12126177e-06, -2.02446914e-06]), array([7.29715274e-15, 1.62298677e-07, 1.90437143e-14])]\n",
      "[array([ -34.45363361, -169.86754414,  -83.38925414]), array([2.56864067e-04, 1.06299842e+01, 4.69508746e-04])]\n",
      "[array([0., 0., 0.]), array([-0., -0., -0.])]\n",
      "[array([-183193.48437783, -452545.86314586, -277458.06993302]), array([3.02715689e-205, 4.49295890e+002, 3.35588137e-193])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n",
      "[array([nan, nan, nan]), array([nan, nan, nan])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3960/1324316319.py:26: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.sum(y * np.log(x))\n",
      "/tmp/ipykernel_3960/1324316319.py:26: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -np.sum(y * np.log(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(nan)]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = model.fit(dataset=train_dataset, n_epoch=1, verbose=0)\n",
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(1.173731299006055)]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.5, 2.3, 4. , 1.3])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([nan, nan, nan]), [0, 1, 0])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[1]), y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.25763174, 0.25763174, 0.25763174]), [0, 0, 1])"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_val[1]), y_val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[392], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample,target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X_train, y_train):\n\u001b[1;32m      5\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(sample\u001b[38;5;241m=\u001b[39msample)\n\u001b[0;32m----> 7\u001b[0m     pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(pred[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# print(pred)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print(target)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(pred) \u001b[38;5;241m==\u001b[39m target:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "true_count = 0\n",
    "all_count = 0\n",
    "\n",
    "for sample,target in zip(X_train, y_train):\n",
    "    pred = model.predict(sample=sample)\n",
    "\n",
    "    pred = np.where(pred[0] >= max(pred[0]), 1, 0)\n",
    "    # print(pred)\n",
    "    # print(target)\n",
    "\n",
    "    if list(pred) == target:\n",
    "        true_count += 1\n",
    "    all_count += 1 \n",
    "\n",
    "\n",
    "acc = true_count / all_count\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax([1,0,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
