{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "type Sample = list[int]\n",
    "type Data = list[Sample]\n",
    "\n",
    "type Label = int\n",
    "type Labels = list[Label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add more funcs\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "\n",
    "class ActivationBase(ABC):\n",
    "    @abstractmethod\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        \"\"\"Apply the activation function to an input\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class ReLU(ActivationBase):\n",
    "    def calc(self, x) -> list[float]:\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "\n",
    "class ELU(ActivationBase):\n",
    "    def __init__(self, alpha: float = 1.0) -> None:\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        return np.where(x > 0, x, self.alpha * (np.exp(x) - 1))\n",
    "\n",
    "\n",
    "class Sigmoid(ActivationBase):\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class Tahn(ActivationBase):\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "        return np.tanh(x)\n",
    "\n",
    "\n",
    "class Softmax(ActivationBase):\n",
    "    def calc(self, x: Sample) -> list[float]:\n",
    "\n",
    "        max_value = np.max(x)\n",
    "        x -= max_value\n",
    "\n",
    "        exp_values = np.exp(x)\n",
    "        return exp_values / np.sum(exp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.39262147e-44, 7.31058579e-01, 9.74950551e-35, 2.68941421e-01])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 100, 22, 99]\n",
    "\n",
    "f = Softmax()\n",
    "f.calc(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data: Data, labels: Labels) -> None:\n",
    "        self.data: Data = data\n",
    "        self._len = len(data)\n",
    "        self.labels: Labels = labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._len\n",
    "    \n",
    "    def __getitem__(self, index) -> Sample:\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_inputs: int, n_neurons: int, activation: ActivationBase) -> None:\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        \n",
    "        self.weights = self._init_weights()\n",
    "        self.biases = self._init_biases()\n",
    "\n",
    "        self.activation = activation\n",
    "    \n",
    "    def _init_weights(self) -> list[float]:\n",
    "        return np.random.randn(self.n_inputs, self.n_neurons) * 0.1\n",
    "    \n",
    "    def _init_biases(self):\n",
    "        return np.random.randn(self.n_inputs, self.n_neurons)\n",
    "    \n",
    "    def forward(self, inputs) -> None:\n",
    "        # print(f\"{inputs = }\")\n",
    "        # print(f\"{self.weights = }\")\n",
    "        # print(f\"{self.biases = }\")\n",
    "        \n",
    "        output = np.dot(inputs, self.weights)\n",
    "\n",
    "        # print(output)\n",
    "\n",
    "        # output += self.biases\n",
    "        output = self.biases + output\n",
    "\n",
    "\n",
    "\n",
    "        self.output = self.activation.calc(output)\n",
    "        print(f\"{output = }\")\n",
    "\n",
    "\n",
    "Layers = list[Layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers: Layers):\n",
    "        self.layers = layers\n",
    "        self._layers_len = len(layers)\n",
    "\n",
    "    def fit(self, dataset: Dataset):\n",
    "        for i,sample in enumerate(dataset):\n",
    "            print(\"_______ Layer\", 1, '\\n\\n')\n",
    "            self.layers[0].forward(inputs=sample)\n",
    "            \n",
    "            for i in range(1, self._layers_len):\n",
    "                print(\"______ Layer\", i+1, '\\n\\n')\n",
    "                self.layers[i].forward(inputs=self.layers[i-1].output)\n",
    "    \n",
    "    def predict(self):\n",
    "        ...\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "            weights.append(layer.weights)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    [1,2,3,4],\n",
    "    # [4,3,2,1]\n",
    "]\n",
    "\n",
    "\n",
    "# train_data = [\n",
    "#     [\n",
    "#         [1,2,3,4],\n",
    "#         [1,2,3,4],\n",
    "#     ],\n",
    "# ]\n",
    "\n",
    "val_data = [\n",
    "    [1,2,3,4],\n",
    "    [4,3,2,1]\n",
    "]\n",
    "\n",
    "\n",
    "train_dataset = Dataset(data=train_data, labels=[1,1])\n",
    "# val_dataset = Dataset(data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Layer(4,1, activation=ReLU()),\n",
    "    Layer(1,1, activation=ReLU()),\n",
    "    Layer(1,5, activation=Softmax()),\n",
    "]\n",
    "\n",
    "model = NeuralNetwork(layers=layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 4],\n",
       "       [2, 3, 4]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3],[1,2,3],])\n",
    "\n",
    "b = np.array([[1,2,3],[1,2,3],])\n",
    "\n",
    "\n",
    "# np.dot(a,b.T)\n",
    "a + [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______ Layer 1 \n",
      "\n",
      "\n",
      "output = array([[-0.31398245],\n",
      "       [ 1.18054483],\n",
      "       [-0.90531665],\n",
      "       [ 0.12439545]])\n",
      "______ Layer 2 \n",
      "\n",
      "\n",
      "output = array([[-0.38354324],\n",
      "       [-0.29798873],\n",
      "       [-0.38354324],\n",
      "       [-0.37452826]])\n",
      "______ Layer 3 \n",
      "\n",
      "\n",
      "output = array([[-0.03876571,  0.        , -1.42527781, -2.70225199, -1.70905178],\n",
      "       [-0.03876571,  0.        , -1.42527781, -2.70225199, -1.70905178],\n",
      "       [-0.03876571,  0.        , -1.42527781, -2.70225199, -1.70905178],\n",
      "       [-0.03876571,  0.        , -1.42527781, -2.70225199, -1.70905178]])\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.6845608 ,  1.45302676],\n",
       "       [ 2.22517856,  0.78169763],\n",
       "       [ 0.97735487, -0.91557542],\n",
       "       [-1.6375041 , -0.2103463 ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.04662128,  0.00362523,  0.11101263,  0.21666945],\n",
       "        [-0.0427738 , -0.02047442,  0.00652623,  0.12543406],\n",
       "        [ 0.04504755, -0.13728407,  0.14258488, -0.1006611 ],\n",
       "        [-0.21504161, -0.09286219,  0.02443559, -0.06513455]]),\n",
       " array([[ 0.01747982,  0.05444045],\n",
       "        [ 0.05884424, -0.05411848],\n",
       "        [-0.07776638,  0.02290638],\n",
       "        [ 0.02842925,  0.12456388]])]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
